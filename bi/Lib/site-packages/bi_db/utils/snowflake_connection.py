import sys
import pandas as pd
import snowflake.connector
from db_connection import dbConnection
from redshift_connection import RedshiftConnection
from bi_exceptions import SnowflakeException
from sqlalchemy import create_engine
from sqlalchemy.pool import NullPool
from snowflake.sqlalchemy import URL
from datetime import datetime
from bi_tools import flex_read
from bi_tools import flex_write

from biz_intel_creds import CredsList
snowflake_creds = CredsList().snowflake

class SnowflakeConnection(object):
    def __init__(self):
        """Sets up connection and class attributes

        Args:
            NA
        Returns:
            NA
        Raises:
            NA
        """
        self.connection = snowflake.connector.connect(
                            user=snowflake_creds['USER'],
                            password=snowflake_creds['PASSWORD'],
                            account=snowflake_creds['ACCOUNT'],
                            role="ACCOUNTADMIN"
                        )
        self.engine = self.connection.cursor()

    def write_to_sql(self, df, db_name, schema_name, table_name, **kwargs):
        """Append to or replace an existing table in snowflake.

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
            table_name: name of the table in snowflake
            kwargs: ["if_exists"]
        Returns:
            NA
        Raises:
            NA
        """
        # check to see if `if_exists` key argument was passed in
        try:
            kwargs["if_exists"]
            if kwargs["if_exists"] not in ["fail", "replace", "append"]:
                raise SnowflakeException("`if_exists` should be one of"\
                    "[`fail`, `replace`, `append`]")
            else:
                if_exists = kwargs["if_exists"]
        except NameError:
            if_exists = "append"
        except KeyError:
            if_exists = "append"

        custom_engine = self._create_custom_engine(db_name, schema_name)
        df = self._format_for_load(df)
        capitalize_columns_dict = {i: i.upper() for i in df.columns.tolist()}
        df = df.rename(columns=capitalize_columns_dict)
        df.to_sql(name=table_name, con=custom_engine, if_exists=if_exists,
            index=False, chunksize=1000)

    def load(self, db_name, schema_name, table_name,
        filepath, format):
        """Load s3 object into Snowflake.

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
            table_name: name of the table in snowflake
            filepath: filepath of the s3 object
            format: format of the s3 object
        Returns:
            NA
        Raises:
            NA
        """
        if format.upper() == "CSV":
            format = "comma_delimited"
        elif format.upper() == "JSON":
            #format = ""
            raise SnowflakeException("format not supported")
        elif format.upper() == "GZIP":
            #format = ""
            raise SnowflakeException("format not supported")
        else:
            raise SnowflakeException("format not supported")

        load_query = """
        COPY INTO {schema}.{table} FROM {filepath}
        FILE_FORMAT = (FORMAT_NAME='{format}')
        ON_ERROR = CONTINUE
        force=true;
                    """.format(schema=schema_name,
                            table=table_name,
                            filepath=filepath.replace(
                                "s3://pitchbook-snowflake",
                                "@pb_snowflake"
                            ),
                            format=format)
        if self._table_exists(table_name, db_name):
            self.query_executor("USE SCHEMA {}.PUBLIC".format(db_name))
            self.query_executor(load_query)
            self.query_executor("COMMIT")
        else:
            if "pitchbook-snowflake" in filepath.lower():
                df = flex_read(filepath, s3=True,
                    bucket_name="pitchbook-snowflake").head(500)
            elif "pb-unified-usage-data-source" in filepath.lower():
                df = flex_read(filepath, s3=True,
                    bucket_name="pb-unified-usage-data-source").head(500)
            self.engine.execute("USE SCHEMA {db_name}.{schema}".format(
                db_name=db_name,
                schema=schema_name
                )
            )
            self.write_to_sql(df=df, db_name=db_name,
                schema_name=schema_name, table_name=table_name,
                if_exists="replace"
            )
            self.query_executor("USE SCHEMA {}.{}".format(
                db_name, schema_name
                )
            )
            self.query_executor("DELETE FROM {}".format(table_name))
            self.query_executor("USE SCHEMA {}.PUBLIC".format(db_name))
            self.query_executor(load_query)
            self.query_executor("COMMIT")
            self._grant_permission(db_name, schema_name)

    def append(self, schema_name, table_name,
        filepath, format="csv", db_name="BUSINESS_INTELLIGENCE"):
        """Bulk append s3 object into Snowflake.
            Can append the same object only once.

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
            table_name: name of the table in snowflake
            filepath: filepath of the s3 object
            format: format of the s3 object
        Returns:
            NA
        Raises:
            NA
        """
        # default bulk load is bulk append
        self.load(db_name, schema_name, table_name,
            filepath, format)

    def update(self, schema_name, table_name, filepath,
        update_on, format="comma_delimited", db_name="BUSINESS_INTELLIGENCE"):
        """Bulk upsert s3 object into Snowflake

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
            table_name: name of the table in snowflake
            update_on: name of the column to update on
            filepath: filepath of the s3 object
            format: format of the s3 object
        Returns:
            NA
        Raises:
            NA
        """
        # create staging_{destination_table_name}
        # and upload to staging
        staging_name = "STAGING_" + table_name
        if "pitchbook-snowflake" in filepath.lower():
            df = flex_read(filepath, s3=True, bucket_name="pitchbook-snowflake")
        elif "pb-unified-usage-data-source" in filepath.lower():
            df = flex_read(filepath, s3=True,
                bucket_name="pb-unified-usage-data-source")
        column_list = df.columns.tolist()
        update_column_match_string = ', '.join(
            "{column} = {schema}.{temp_table}.{column}".format(
            column=i, temp_table=staging_name, schema=schema_name) for i in column_list)
        temp_column_string = ', '.join(
            "{schema}.{temp_table}.{column}".format(
            column=i, temp_table=staging_name, schema=schema_name) for i in column_list)
        prod_column_string = ', '.join(
            "{column}".format(
            column=i) for i in column_list)
        self.query_executor("USE DATABASE {}".format(db_name))
        create_temp_table_query = """CREATE TEMPORARY TABLE {schema}.{temp_table}
            LIKE {schema}.{table};""".format(
                schema=schema_name
                , table=table_name
                , temp_table=staging_name)
        load_temp_query = """COPY INTO {schema}.{temp_table}({prod_columns})
            FROM {filepath}
            FILE_FORMAT = (FORMAT_NAME='{format_name}');""".format(
                schema=schema_name
                , temp_table=staging_name
                , prod_columns=prod_column_string
                , filepath=filepath.replace(
                    "s3://pitchbook-snowflake",
                    "@business_intelligence")
                , format_name=format)
        merge_query = """MERGE INTO {schema}.{table}
            USING {schema}.{temp_table}
            ON {schema}.{table}.{update_on} = {schema}.{temp_table}.{update_on}
            WHEN MATCHED THEN UPDATE SET {update_column_match_string}
            WHEN NOT MATCHED THEN INSERT({prod_columns}) VALUES({temp_columns});""".format(
                schema=schema_name
                , table=table_name
                , temp_table=staging_name
                , update_on=update_on
                , update_column_match_string=update_column_match_string
                , prod_columns=prod_column_string
                , temp_columns=temp_column_string)
        self.query_executor("USE SCHEMA {}.PUBLIC;".format(db_name))
        self.query_executor(create_temp_table_query)
        self.query_executor(load_temp_query)
        self.query_executor(merge_query)

    def replace(self, schema_name, table_name,
        filepath, format="csv", db_name="BUSINESS_INTELLIGENCE"):
        """Bulk replace s3 object into Snowflake.
            Can replace with the same s3 object more than once.

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
            table_name: name of the table in snowflake
            filepath: filepath of the s3 object
            format: format of the s3 object
        Returns:
            NA
        Raises:
            NA
        """
        if self._table_exists(table_name, db_name):
            self.query_executor("USE DATABASE {}".format(db_name))
            self.query_executor("DELETE FROM {}.{}".format(
                schema_name, table_name
                )
            )
        self.load(db_name, schema_name, table_name,
            filepath, format
        )

    def create(self, object_name, object_type=None, **kwargs):
        """Create a database object.

        Args:
            object_name: name of the object you are creating
            object_type: (DATABASE, SCHEMA, WAREHOUSE, TABLE)
            DATABASE(key argument): name of the database
            SCHEMA(key argument): name of the schema
            df(key argument): dataframe you want to create the table with
        Returns:
            NA
        Raises:
            NA
        """
        if "DATABASE" in kwargs:
            db_name = kwargs["DATABASE"]
            if "SCHEMA" in kwargs:
                schema_name = kwargs["SCHEMA"]

        if object_type is None:
            raise SnowflakeException("object type must be one of "\
                                        "['DATABASE', 'SCHEMA'," \
                                        "'WAREHOUSE', 'TABLE']"
                                    )
        elif object_type.upper() in ["DATABASE", "WAREHOUSE"]:
            self.engine.execute("CREATE {ot} IF NOT EXISTS {on}".format(
                ot=object_type, on=object_name
                )
            )
        elif object_type.upper() == "SCHEMA":
            self.engine.execute("USE DATABASE {db_name}".format(
                db_name=kwargs["DATABASE"]
                )
            )
            self.engine.execute("CREATE {ot} IF NOT EXISTS {on}".format(
                ot=object_type, on=object_name
                )
            )
            self._grant_permission(kwargs["DATABASE"], object_name)
        elif object_type.upper() == "TABLE":
            self.engine.execute("USE SCHEMA {db_name}.{schema}".format(
                db_name=kwargs["DATABASE"],
                schema=kwargs["SCHEMA"]
                )
            )
            # default append creates the table
            today = datetime.strftime(datetime.today(), "%Y-%m-%d")
            savepath = \
            "s3://pitchbook-snowflake/schema={schema}"\
            "/table={table}/{today}/{schema}_{table}.csv".format(
                schema=kwargs["SCHEMA"], table=object_name, today=today)

            flex_write(kwargs["df"], savepath,s3=True)
            self.append(schema_name=kwargs["SCHEMA"], table_name=object_name,
                filepath=savepath)

    def unload(self, database, schema, table, **kwargs):
        """Unload a database table into a specified s3 location

        Args:
            database: name of the database
            schema_name: name of the schema
            table_name: name of the table
            s3_path: filepath of the s3 object
            kwargs: ["aws_access_key_id", "aws_secret_access_key"]
        Returns:
            NA
        Raises:
            NA
        """
        schema = schema.upper()
        table = table.upper()
        try:
            kwargs["s3_path"]
            s3_path = kwargs["s3_path"]
        except NameError:
            today = datetime.strftime(datetime.today(), "%Y-%m-%d")
            s3_path = "s3://pitchbook-snowflake/schema={schema}/table={table}/"\
                "{today}".format(schema=schema, table=table, today=today)
        except KeyError:
            today = datetime.strftime(datetime.today(), "%Y-%m-%d")
            s3_path = "s3://pitchbook-snowflake/schema={schema}/table={table}/"\
                "{today}".format(schema=schema, table=table, today=today)
        if set(["aws_access_key_id", "aws_secret_access_key"]) < \
            set(list(kwargs)):
            aws_key_given = True
        else:
            aws_key_given = False
        if not aws_key_given:
            sql="""
        UNLOAD \
('select * from {database}.{schema}.{table}') TO '{s3path}/{schema}_{table}' \
        CREDENTIALS
            'aws_iam_role=arn:aws:iam::542960883369:role/redshift_access_role' \
        DELIMITER AS ',' \
        ADDQUOTES \
        NULL AS '' \
        ALLOWOVERWRITE \
        PARALLEL OFF;"""\
            .format(database=database,
                   schema=schema,
                   table=table,
                   s3path=s3_path)
        elif aws_key_given:
            sql="""
        UNLOAD \
('select * from {database}.{schema}.{table}') TO '{s3path}/{schema}_{table}' \
        CREDENTIALS
            'aws_access_key_id={aki};aws_secret_access_key={sck}' \
        DELIMITER AS ',' \
        ADDQUOTES \
        NULL AS '' \
        ALLOWOVERWRITE \
        PARALLEL OFF;"""\
            .format(database=database,
                   schema=schema,
                   table=table,
                   s3path=s3_path,
                   aki=kwargs["aws_access_key_id"],
                   sck=kwargs["aws_secret_access_key"])
        #logger.custom_log("Unloading your table")
        self.query_executor(sql)

        df = self.sql_dataframe("select * from {}.{}.{} limit 3;".format(
                database, schema, table))
        df = pd.DataFrame(df.columns)
        df.rename(columns={0:"column_name"}, inplace=True)
        flex_write(df, s3_path + "/column_names.csv", "csv", s3=True)


    def get_metadata(self, db_name, schema_name, table_name):
        """Get and return the metadata table.

        Args:
            database: name of the database
            schema: name of the schema
            table: name of the table
        Returns:
            metadata: list of metadata of each field
        Raises:
            NA
        """
        self.engine.execute("SELECT * FROM {}.{}.{} limit 5"\
            .format(db_name, schema_name, table_name))
        return ','.join([col[0] for col in self.engine.description])

    def get_query_id(self, query_order=-1):
        """Get and return the query_id.

        Args:
            query_order: the order of the query_id being fetched
        Returns:
            query_id: the id the of the query
        Raises:
            SnowflakeException
        """
        df = pd.read_sql(sql="select last_query_id({ord})"\
                        .format(ord=query_order),con=self.connection)
        return df["LAST_QUERY_ID({ord})".format(ord=query_order)][0]

    def cancel_query(self, query_id):
        """Cancel the query of the given query_id.

        Args:
            query_id: the of query you want to cancel
        Returns:
            NA
        Raises:
            SnowflakeException
        """
        try:
            self.engine.execute(r"select SYSTEM$CANCEL_QUERY('{queryID}')"\
               .format(queryID=query_id))
        except:
            raise SnowflakeException("Cannot cancel query_id:{}"\
                .format(query_id))

    def query_executor(self, query):
        """Executes the query.

        Args:
            query: query to execute
        Returns:
            NA
        Raises:
            NA
        """
        self.engine.execute(query)

    def sql_dataframe(self, query):
        """Execute the query and return the queried results
           in a pandas dataframe.

        Args:
            query: query to execute
        Returns:
            df_result: pandas DataFrame of the queried result
        Raises:
            NA
        """
        try:
            df_result = pd.read_sql(query, self.connection)
        except TypeError:
            df_result = pd.read_sql(query.replace("%", "%%"), self.connection)
        return df_result

    def _create_custom_engine(self, db_name, schema_name):
        """Create custom engine to Snowflake.

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
        Returns:
            NA
        Raises:
            NA
        """
        url = URL(account=snowflake_creds["ACCOUNT"],
                user=snowflake_creds["USER"],
                password=snowflake_creds["PASSWORD"],
                role="ACCOUNTADMIN",
                database=db_name,
                schema=schema_name,
                numpy=True)
        custom_engine = create_engine(url, poolclass=NullPool)
        return custom_engine

    def _table_exists(self, table_name, db_name):
        """Check to see if table exists.

        Args:
            table_name: name of the table in snowflake
        Returns:
            table_exists: boolean result of whether table exists or not
        Raises:
            NA
        """
        self.query_executor("USE WAREHOUSE PB_DW")
        self.query_executor("USE DATABASE {}".format(db_name))
        df = self.sql_dataframe(
        "SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE'")
        if table_name in df.TABLE_NAME.unique():
            return True
        else:
            return False

    def _close_connection(self):
        """Close the open connection to Snowflake db.

        Args:
            NA
        Returns:
            NA
        Raises:
            NA
        """
        self.connection.close()

    def _drop_table(self, db_name, schema_name, table_name):
        """Drop a table from Snowflake db.

        Args:
            NA
        Returns:
            NA
        Raises:
            NA
        """
        self.query_executor("USE SCHEMA {}.{}".format(db_name,schema_name))
        self.query_executor("DROP TABLE {}".format(table_name))

    def _grant_permission(self, db_name, schema_name, **kwargs):
        """Grant permission to database objects.

        Args:
            db_name:
            schema_name:
            table_name:
            role_name:
        Returns:
            NA
        Raises:
            NA
        """
        if "role" in kwargs:
            role = kwargs["role"]
        else:
            role = "BI_READ_ONLY"

        self.query_executor("USE DATABASE {}".format(db_name))
        self.query_executor(
            "grant usage on schema {} to role {};".format(schema_name, role))
        self.query_executor(
           "grant all on all tables in schema {} to role {};".format(
                schema_name, role))

    def _format_for_load(self, df):
        try:
            datetime_cols = [x for x in df.columns if "_date" in x.lower()]
        except AttributeError as e:
            new_header = df.iloc[0]
            df = df[1:]
            df.columns = new_header
            datetime_cols = [x for x in df.columns if "_date" in x.lower()]
        except:
            raise ValueError("check your s3 object input")
        for col in datetime_cols:
            df[col] = pd.to_datetime(df[col], errors = 'coerce')
        # if all values for the given column is na, then set it to string
        for col in df.columns:
            if df[col].isnull().all():
                df[col] = df[col].astype(str)
        return df

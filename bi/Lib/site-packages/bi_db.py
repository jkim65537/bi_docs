import sys
import pandas as pd
import snowflake.connector
from db_connection import dbConnection
from redshift_connection import RedshiftConnection
from bi_exceptions import SnowflakeException
from sqlalchemy import create_engine
from sqlalchemy.pool import NullPool
from snowflake.sqlalchemy import URL
from datetime import datetime
from bi_tools import flex_read
from bi_tools import flex_write

from biz_intel_creds import CredsList
snowflake_creds = CredsList().snowflake

class SnowflakeConnection(object):
    def __init__(self):
        """Sets up connection and class attributes

        Args:
            NA
        Returns:
            NA
        Raises:
            NA
        """
        self.connection = snowflake.connector.connect(
                            user=snowflake_creds['USER'],
                            password=snowflake_creds['PASSWORD'],
                            account=snowflake_creds['ACCOUNT'],
                            role="ACCOUNTADMIN"
                        )
        self.engine = self.connection.cursor()

    def write_to_sql(self, df, db_name, schema_name, table_name, **kwargs):
        """Append to or replace an existing table in snowflake.

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
            table_name: name of the table in snowflake
            kwargs: ["if_exists"]
        Returns:
            NA
        Raises:
            NA
        """
        # check to see if `if_exists` key argument was passed in
        try:
            kwargs["if_exists"]
            if kwargs["if_exists"] not in ["fail", "replace", "append"]:
                raise SnowflakeException("`if_exists` should be one of"\
                    "[`fail`, `replace`, `append`]")
            else:
                if_exists = kwargs["if_exists"]
        except NameError:
            if_exists = "append"
        except KeyError:
            if_exists = "append"

        custom_engine = self._create_custom_engine(db_name, schema_name)
        df = self._format_for_load(df)
        capitalize_columns_dict = {i: i.upper() for i in df.columns.tolist()}
        df = df.rename(columns=capitalize_columns_dict)
        df.to_sql(name=table_name, con=custom_engine, if_exists=if_exists,
            index=False, chunksize=1000)

    def load(self, db_name, schema_name, table_name,
        filepath, format):
        """Load s3 object into Snowflake.

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
            table_name: name of the table in snowflake
            filepath: filepath of the s3 object
            format: format of the s3 object
        Returns:
            NA
        Raises:
            NA
        """
        if format.upper() == "CSV":
            format = "comma_delimited"
        elif format.upper() == "JSON":
            #format = ""
            raise SnowflakeException("format not supported")
        elif format.upper() == "GZIP":
            #format = ""
            raise SnowflakeException("format not supported")
        else:
            raise SnowflakeException("format not supported")

        load_query = """
        COPY INTO {schema}.{table} FROM {filepath}
        FILE_FORMAT = (FORMAT_NAME='{format}')
        ON_ERROR = CONTINUE
        force=true;
                    """.format(schema=schema_name,
                            table=table_name,
                            filepath=filepath.replace(
                                "s3://pitchbook-snowflake",
                                "@pb_snowflake"
                            ),
                            format=format)
        if self._table_exists(table_name, db_name):
            self.query_executor("USE SCHEMA {}.PUBLIC".format(db_name))
            self.query_executor(load_query)
            self.query_executor("COMMIT")
        else:
            if "pitchbook-snowflake" in filepath.lower():
                df = flex_read(filepath, s3=True,
                    bucket_name="pitchbook-snowflake").head(500)
            elif "pb-unified-usage-data-source" in filepath.lower():
                df = flex_read(filepath, s3=True,
                    bucket_name="pb-unified-usage-data-source").head(500)
            self.engine.execute("USE SCHEMA {db_name}.{schema}".format(
                db_name=db_name,
                schema=schema_name
                )
            )
            self.write_to_sql(df=df, db_name=db_name,
                schema_name=schema_name, table_name=table_name,
                if_exists="replace"
            )
            self.query_executor("USE SCHEMA {}.{}".format(
                db_name, schema_name
                )
            )
            self.query_executor("DELETE FROM {}".format(table_name))
            self.query_executor("USE SCHEMA {}.PUBLIC".format(db_name))
            self.query_executor(load_query)
            self.query_executor("COMMIT")
            self._grant_permission(db_name, schema_name)

    def append(self, schema_name, table_name,
        filepath, format="csv", db_name="BUSINESS_INTELLIGENCE"):
        """Bulk append s3 object into Snowflake.
            Can append the same object only once.

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
            table_name: name of the table in snowflake
            filepath: filepath of the s3 object
            format: format of the s3 object
        Returns:
            NA
        Raises:
            NA
        """
        # default bulk load is bulk append
        self.load(db_name, schema_name, table_name,
            filepath, format)

    def update(self, schema_name, table_name, filepath,
        update_on, format="comma_delimited", db_name="BUSINESS_INTELLIGENCE"):
        """Bulk upsert s3 object into Snowflake

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
            table_name: name of the table in snowflake
            update_on: name of the column to update on
            filepath: filepath of the s3 object
            format: format of the s3 object
        Returns:
            NA
        Raises:
            NA
        """
        # create staging_{destination_table_name}
        # and upload to staging
        staging_name = "STAGING_" + table_name
        if "pitchbook-snowflake" in filepath.lower():
            df = flex_read(filepath, s3=True, bucket_name="pitchbook-snowflake")
        elif "pb-unified-usage-data-source" in filepath.lower():
            df = flex_read(filepath, s3=True,
                bucket_name="pb-unified-usage-data-source")
        column_list = df.columns.tolist()
        update_column_match_string = ', '.join(
            "{column} = {schema}.{temp_table}.{column}".format(
            column=i, temp_table=staging_name, schema=schema_name) for i in column_list)
        temp_column_string = ', '.join(
            "{schema}.{temp_table}.{column}".format(
            column=i, temp_table=staging_name, schema=schema_name) for i in column_list)
        prod_column_string = ', '.join(
            "{column}".format(
            column=i) for i in column_list)
        self.query_executor("USE DATABASE {}".format(db_name))
        create_temp_table_query = """CREATE TEMPORARY TABLE {schema}.{temp_table}
            LIKE {schema}.{table};""".format(
                schema=schema_name
                , table=table_name
                , temp_table=staging_name)
        load_temp_query = """COPY INTO {schema}.{temp_table}({prod_columns})
            FROM {filepath}
            FILE_FORMAT = (FORMAT_NAME='{format_name}');""".format(
                schema=schema_name
                , temp_table=staging_name
                , prod_columns=prod_column_string
                , filepath=filepath.replace(
                    "s3://pitchbook-snowflake",
                    "@business_intelligence")
                , format_name=format)
        merge_query = """MERGE INTO {schema}.{table}
            USING {schema}.{temp_table}
            ON {schema}.{table}.{update_on} = {schema}.{temp_table}.{update_on}
            WHEN MATCHED THEN UPDATE SET {update_column_match_string}
            WHEN NOT MATCHED THEN INSERT({prod_columns}) VALUES({temp_columns});""".format(
                schema=schema_name
                , table=table_name
                , temp_table=staging_name
                , update_on=update_on
                , update_column_match_string=update_column_match_string
                , prod_columns=prod_column_string
                , temp_columns=temp_column_string)
        self.query_executor("USE SCHEMA {}.PUBLIC;".format(db_name))
        self.query_executor(create_temp_table_query)
        self.query_executor(load_temp_query)
        self.query_executor(merge_query)

    def replace(self, schema_name, table_name,
        filepath, format="csv", db_name="BUSINESS_INTELLIGENCE"):
        """Bulk replace s3 object into Snowflake.
            Can replace with the same s3 object more than once.

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
            table_name: name of the table in snowflake
            filepath: filepath of the s3 object
            format: format of the s3 object
        Returns:
            NA
        Raises:
            NA
        """
        if self._table_exists(table_name, db_name):
            self.query_executor("USE DATABASE {}".format(db_name))
            self.query_executor("DELETE FROM {}.{}".format(
                schema_name, table_name
                )
            )
        self.load(db_name, schema_name, table_name,
            filepath, format
        )

    def create(self, object_name, object_type=None, **kwargs):
        """Create a database object.

        Args:
            object_name: name of the object you are creating
            object_type: (DATABASE, SCHEMA, WAREHOUSE, TABLE)
            DATABASE(key argument): name of the database
            SCHEMA(key argument): name of the schema
            df(key argument): dataframe you want to create the table with
        Returns:
            NA
        Raises:
            NA
        """
        if "DATABASE" in kwargs:
            db_name = kwargs["DATABASE"]
            if "SCHEMA" in kwargs:
                schema_name = kwargs["SCHEMA"]

        if object_type is None:
            raise SnowflakeException("object type must be one of "\
                                        "['DATABASE', 'SCHEMA'," \
                                        "'WAREHOUSE', 'TABLE']"
                                    )
        elif object_type.upper() in ["DATABASE", "WAREHOUSE"]:
            self.engine.execute("CREATE {ot} IF NOT EXISTS {on}".format(
                ot=object_type, on=object_name
                )
            )
        elif object_type.upper() == "SCHEMA":
            self.engine.execute("USE DATABASE {db_name}".format(
                db_name=kwargs["DATABASE"]
                )
            )
            self.engine.execute("CREATE {ot} IF NOT EXISTS {on}".format(
                ot=object_type, on=object_name
                )
            )
            self._grant_permission(kwargs["DATABASE"], object_name)
        elif object_type.upper() == "TABLE":
            self.engine.execute("USE SCHEMA {db_name}.{schema}".format(
                db_name=kwargs["DATABASE"],
                schema=kwargs["SCHEMA"]
                )
            )
            # default append creates the table
            today = datetime.strftime(datetime.today(), "%Y-%m-%d")
            savepath = \
            "s3://pitchbook-snowflake/schema={schema}"\
            "/table={table}/{today}/{schema}_{table}.csv".format(
                schema=kwargs["SCHEMA"], table=object_name, today=today)

            flex_write(kwargs["df"], savepath,s3=True)
            self.append(schema_name=kwargs["SCHEMA"], table_name=object_name,
                filepath=savepath)

    def unload(self, database, schema, table, **kwargs):
        """Unload a database table into a specified s3 location

        Args:
            database: name of the database
            schema_name: name of the schema
            table_name: name of the table
            s3_path: filepath of the s3 object
            kwargs: ["aws_access_key_id", "aws_secret_access_key"]
        Returns:
            NA
        Raises:
            NA
        """
        schema = schema.upper()
        table = table.upper()
        try:
            kwargs["s3_path"]
            s3_path = kwargs["s3_path"]
        except NameError:
            today = datetime.strftime(datetime.today(), "%Y-%m-%d")
            s3_path = "s3://pitchbook-snowflake/schema={schema}/table={table}/"\
                "{today}".format(schema=schema, table=table, today=today)
        except KeyError:
            today = datetime.strftime(datetime.today(), "%Y-%m-%d")
            s3_path = "s3://pitchbook-snowflake/schema={schema}/table={table}/"\
                "{today}".format(schema=schema, table=table, today=today)
        if set(["aws_access_key_id", "aws_secret_access_key"]) < \
            set(list(kwargs)):
            aws_key_given = True
        else:
            aws_key_given = False
        if not aws_key_given:
            sql="""
        UNLOAD \
('select * from {database}.{schema}.{table}') TO '{s3path}/{schema}_{table}' \
        CREDENTIALS
            'aws_iam_role=arn:aws:iam::542960883369:role/redshift_access_role' \
        DELIMITER AS ',' \
        ADDQUOTES \
        NULL AS '' \
        ALLOWOVERWRITE \
        PARALLEL OFF;"""\
            .format(database=database,
                   schema=schema,
                   table=table,
                   s3path=s3_path)
        elif aws_key_given:
            sql="""
        UNLOAD \
('select * from {database}.{schema}.{table}') TO '{s3path}/{schema}_{table}' \
        CREDENTIALS
            'aws_access_key_id={aki};aws_secret_access_key={sck}' \
        DELIMITER AS ',' \
        ADDQUOTES \
        NULL AS '' \
        ALLOWOVERWRITE \
        PARALLEL OFF;"""\
            .format(database=database,
                   schema=schema,
                   table=table,
                   s3path=s3_path,
                   aki=kwargs["aws_access_key_id"],
                   sck=kwargs["aws_secret_access_key"])
        #logger.custom_log("Unloading your table")
        self.query_executor(sql)

        df = self.sql_dataframe("select * from {}.{}.{} limit 3;".format(
                database, schema, table))
        df = pd.DataFrame(df.columns)
        df.rename(columns={0:"column_name"}, inplace=True)
        flex_write(df, s3_path + "/column_names.csv", "csv", s3=True)


    def get_metadata(self, db_name, schema_name, table_name):
        """Get and return the metadata table.

        Args:
            database: name of the database
            schema: name of the schema
            table: name of the table
        Returns:
            metadata: list of metadata of each field
        Raises:
            NA
        """
        self.engine.execute("SELECT * FROM {}.{}.{} limit 5"\
            .format(db_name, schema_name, table_name))
        return ','.join([col[0] for col in self.engine.description])

    def get_query_id(self, query_order=-1):
        """Get and return the query_id.

        Args:
            query_order: the order of the query_id being fetched
        Returns:
            query_id: the id the of the query
        Raises:
            SnowflakeException
        """
        df = pd.read_sql(sql="select last_query_id({ord})"\
                        .format(ord=query_order),con=self.connection)
        return df["LAST_QUERY_ID({ord})".format(ord=query_order)][0]

    def cancel_query(self, query_id):
        """Cancel the query of the given query_id.

        Args:
            query_id: the of query you want to cancel
        Returns:
            NA
        Raises:
            SnowflakeException
        """
        try:
            self.engine.execute(r"select SYSTEM$CANCEL_QUERY('{queryID}')"\
               .format(queryID=query_id))
        except:
            raise SnowflakeException("Cannot cancel query_id:{}"\
                .format(query_id))

    def query_executor(self, query):
        """Executes the query.

        Args:
            query: query to execute
        Returns:
            NA
        Raises:
            NA
        """
        self.engine.execute(query)

    def sql_dataframe(self, query):
        """Execute the query and return the queried results
           in a pandas dataframe.

        Args:
            query: query to execute
        Returns:
            df_result: pandas DataFrame of the queried result
        Raises:
            NA
        """
        try:
            df_result = pd.read_sql(query, self.connection)
        except TypeError:
            df_result = pd.read_sql(query.replace("%", "%%"), self.connection)
        return df_result

    def _create_custom_engine(self, db_name, schema_name):
        """Create custom engine to Snowflake.

        Args:
            db_name: name of the database in snowflake
            schema_name: name of the schema in snowflake
        Returns:
            NA
        Raises:
            NA
        """
        url = URL(account=snowflake_creds["ACCOUNT"],
                user=snowflake_creds["USER"],
                password=snowflake_creds["PASSWORD"],
                role="ACCOUNTADMIN",
                database=db_name,
                schema=schema_name,
                numpy=True)
        custom_engine = create_engine(url, poolclass=NullPool)
        return custom_engine

    def _table_exists(self, table_name, db_name):
        """Check to see if table exists.

        Args:
            table_name: name of the table in snowflake
        Returns:
            table_exists: boolean result of whether table exists or not
        Raises:
            NA
        """
        self.query_executor("USE WAREHOUSE PB_DW")
        self.query_executor("USE DATABASE {}".format(db_name))
        df = self.sql_dataframe(
        "SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE'")
        if table_name in df.TABLE_NAME.unique():
            return True
        else:
            return False

    def _close_connection(self):
        """Close the open connection to Snowflake db.

        Args:
            NA
        Returns:
            NA
        Raises:
            NA
        """
        self.connection.close()

    def _drop_table(self, db_name, schema_name, table_name):
        """Drop a table from Snowflake db.

        Args:
            NA
        Returns:
            NA
        Raises:
            NA
        """
        self.query_executor("USE SCHEMA {}.{}".format(db_name,schema_name))
        self.query_executor("DROP TABLE {}".format(table_name))

    def _grant_permission(self, db_name, schema_name, **kwargs):
        """Grant permission to database objects.

        Args:
            db_name:
            schema_name:
            table_name:
            role_name:
        Returns:
            NA
        Raises:
            NA
        """
        if "role" in kwargs:
            role = kwargs["role"]
        else:
            role = "BI_READ_ONLY"

        self.query_executor("USE DATABASE {}".format(db_name))
        self.query_executor(
            "grant usage on schema {} to role {};".format(schema_name, role))
        self.query_executor(
           "grant all on all tables in schema {} to role {};".format(
                schema_name, role))

    def _format_for_load(self, df):
        try:
            datetime_cols = [x for x in df.columns if "_date" in x.lower()]
        except AttributeError as e:
            new_header = df.iloc[0]
            df = df[1:]
            df.columns = new_header
            datetime_cols = [x for x in df.columns if "_date" in x.lower()]
        except:
            raise ValueError("check your s3 object input")
        for col in datetime_cols:
            df[col] = pd.to_datetime(df[col], errors = 'coerce')
        # if all values for the given column is na, then set it to string
        for col in df.columns:
            if df[col].isnull().all():
                df[col] = df[col].astype(str)
        return df

# load packages
import pandas as pd
import sys
import getpass
from sqlalchemy import create_engine
from bi_exceptions import RedshiftException
from db_connection import dbConnection
from datetime import datetime
from bi_tools import flex_write

reload(sys)
sys.setdefaultencoding('utf8')

from biz_intel_creds import CredsList
redshift_creds = CredsList().rs_dw

global username
username = getpass.getuser()

def list_to_string(insert_list):
    """Transforms a list into a string.

    Args:
        insert_list: the list to transform into a string
    Returns:
        transformed list
    Raises:
        NA
    """
    field_string = ', '.join("{0}".format(i) for i in insert_list)
    return field_string

class RedshiftConnection(dbConnection):
    def __init__(self):
        ctype = "postgresql+psycopg2://"
        port_num = "5439"
        super(RedshiftConnection, self).__init__(connection_type=ctype,
                                                creds=redshift_creds,
                                                port=port_num)
    def unload(self, database, schema, table, **kwargs):
        """Unload a database table into a specified s3 location

        Args:
            database: name of the database
            schema_name: name of the schema
            table_name: name of the table
            s3_path: filepath of the s3 object
            kwargs: ["aws_access_key_id", "aws_secret_access_key"]
        Returns:
            NA
        Raises:
            NA
        """
        schema = schema.upper()
        table = table.upper()
        try:
            kwargs["s3_path"]
            s3_path = kwargs["s3_path"]
        except NameError:
            today = datetime.strftime(datetime.today(), "%Y-%m-%d")
            s3_path = "s3://pitchbook-snowflake/schema={schema}/table={table}/"\
                "{today}".format(schema=schema, table=table, today=today)
        except KeyError:
            today = datetime.strftime(datetime.today(), "%Y-%m-%d")
            s3_path = "s3://pitchbook-snowflake/schema={schema}/table={table}/"\
                "{today}".format(schema=schema, table=table, today=today)

        if set(["aws_access_key_id", "aws_secret_access_key"]) < \
            set(list(kwargs)):
            aws_key_given = True
        else:
            aws_key_given = False

        if not aws_key_given:
            sql="""
        UNLOAD \
('select * from {database}.{schema}.{table}') TO '{s3path}/{schema}_{table}' \
        CREDENTIALS
            'aws_iam_role=arn:aws:iam::542960883369:role/redshift_access_role' \
        DELIMITER AS ',' \
        ADDQUOTES \
        NULL AS '' \
        ALLOWOVERWRITE \
        PARALLEL OFF;"""\
            .format(database=database,
                   schema=schema,
                   table=table,
                   s3path=s3_path)
        elif aws_key_given:
            sql="""
        UNLOAD \
('select * from {database}.{schema}.{table}') TO '{s3path}/{schema}_{table}' \
        CREDENTIALS
            'aws_access_key_id={aki};aws_secret_access_key={sck}' \
        DELIMITER AS ',' \
        ADDQUOTES \
        NULL AS '' \
        ALLOWOVERWRITE \
        PARALLEL OFF;"""\
            .format(database=database,
                   schema=schema,
                   table=table,
                   s3path=s3_path,
                   aki=kwargs["aws_access_key_id"],
                   sck=kwargs["aws_secret_access_key"])
        #logger.custom_log("Unloading your table")
        self.query_executor(sql)

        df = self.sql_dataframe("select * from {}.{}.{} limit 3;".format(
                database, schema, table))
        df = pd.DataFrame(df.columns)
        df.rename(columns={0:"column_name"}, inplace=True)
        flex_write(df, s3_path + "/column_names.csv", "csv", s3=True)

    def get_metadata_table(self, table_name):
        query = """
                SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTh
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_NAME = '{}'
                """.format(table_name)
        df = self.sql_dataframe(query)
        if len(df) == 0:
            raise RedshiftException("{} does not exist".format(table_name))
        return df

    def make_table(self, df, table_name, schema_name):
        final_df = pd.DataFrame()
        dftype = dict(df.dtypes)
        strings = dict((k, v) for k, v in dftype.items() if v == object)
        numbers = dict((k, v) for k, v in dftype.items() if v != object)

        for col in strings:
            df["len"] = df[col].str.len()
            df_to_append = df.sort_values(by="len", ascending=False)
            final_df.append(df_to_append[:1])

        for col in numbers:
            max_num = df[col].idxmax(axis=1)
            df_to_append = df.loc[df[col] == max_num]
            final_df.append(df_to_append[:1])

        del final_df["len"]

        final_df.to_sql(name=table_name, con=self.connection,
                         schema=schema_name, index = False,
                         if_exists = 'create', chunksize=10000)

    def query_executor(self, query):
        """Executes the specified query.

        Args:
            query: query to execute
        Returns:
            NA
        Raises:
            NA
        """
        self.connection.execute(query)

    def _table_exists(self, table_name):
        """Check whether a given table exists in a given database schema.

        Args:
            schema_name: name of the database schema
            table_name: name of the table
        Returns:
            table_exists: boolean
        Raises:
            NA
        """
        exist_query = "SELECT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES" \
        " where table_name = '{table_name}')".format(table_name=table_name)
        df = pd.read_sql(exist_query, self.connection)
        table_exists = df['?column?'][0]
        return table_exists

    def _grant_permission(self, table_name, schema_name):
        query = "GRANT SELECT ON {sn}.{tn} TO GROUP ro_group;".\
                format(tn=table_name, sn=schema_name)
        self.query_executor(query)

    def drop_table(self, table_name, schema_name):
        """Drop the specified table in the specified schema.

        Args:
            table_name: name of the table that needs to be dropped
            schema_name: name of the schema that the table_name is in
        Returns:
            NA
        Raises:
            NA
        """
        drop_query = """SET SEARCH_PATH TO {0}; DROP TABLE IF EXISTS {1}""".\
                        format(schema_name, table_name)
        self.connection.execute(drop_query)
        self.connection.close()

    def create_table(self, table_name, schema_name, df=None):
        """Insert table.

        Args:
            df: pandas dataframe to upload
            table_name: name of the table to insert
            schema_name: name of the schema the table is in
        Returns:
            NA
        Raises:
            RedshiftException: Table already exists, so replace_table function
                            should be used instead
        """

        #if not given a dataframe, then use an empty dataframe to create table
        if df==None:
            df = pd.DataFrame()
        if int(str(pd.__version__).replace(".", "")) <= 181:
            raise RedshiftException("Update your python version to use this func")

        if self._table_exists("table_name"):
            raise RedshiftException("Table already exists."\
                            "Please use the replace_table function instead.")
        else:
            df.to_sql(name=table_name, con= self.connection, schema=schema_name,\
                    index=False, if_exists="fail", chunksize=500)
            print "{table} successfully created.".format(table=table_name)
            self._grant_permission(table_name, schema_name)

    def replace_table(self, df, table_name, schema_name):
        """Replace an existing table.

        Args:
            df: pandas dataframe to upload
            table_name: name of the table to replace
            schema_name: name of the schema the table is in
        Returns:
            NA
        Raises:
            NA
        """
        if int(str(pd.__version__).replace(".", "")) <= 181:
            raise RedshiftException("Update your python version to use this func")
        df.to_sql(name=table_name, con=self.connection, schema=schema_name,\
                index=False, if_exists="replace", chunksize=500)
        print "{table} successfully replaced.".format(table=table_name)
        self._grant_permission(table_name, schema_name)

    def update_table(self, df, table_name, schema_name):
        """Update the sql data table of your choice

        Args:
            df: pandas dataframe to upload
            table_name: name of the table to insert
            schema_name: name of the schema the table is in
        Returns:
            NA
        Raises:
            NA
        """
        if int(str(pd.__version__).replace(".", "")) <= 181:
            raise RedshiftException("Update your python version to use this func")
        print "Updating table..."
        df.to_sql(name=table_name, con=self.connection, schema=schema_name,\
                index=False, if_exists="append", chunksize=500)
        print "{table} successfully updated.".format(table=table_name)
        self._grant_permission(table_name, schema_name)


    def change_table_name(self, schema_name, old_table_name, new_table_name):
        """Change the table name of your choice.

        Args:
            schema_name: name of the schema the table you want to change the
                        name of is in
            old_table_name: name of the table that you want to change
            new_table_name: new name of the table
        Returns:
            NA
        Raises:
            NA
        """

        table_name_change_query = """alter table {schema}.{old} rename
                                    to {new};
                                    """.format(schema=schema_name,
                                               old=old_table_name,
                                               new=new_table_name)
        self.connection.execute(table_name_change_query)
        self.connection.close()

    def csv_s3_redshift(self, insert_dataframe, table_name, schema_name):
        """Saves the CSV, uploads the CSV to S3, and updates the table within
            Redshift.

        Args:
            query: query to execute
        Returns:
            NA
        Raises:
            InternalError: when the table named after `table_name` does not
                           exist in `scehma_name`
        """
        filename = table_name + ".csv"
        filepath = "/home/{username}/csv/{filename}".format(username=username,
                                                            filename=filename)
        bucket_name = "redshiftstoragetransfer"
        print "Writing the CSV..."
        insert_dataframe.to_csv(filepath, header=None, index=False)
        s3 = boto3.client('s3')
        print "Uploading the CSV to S3"
        s3.upload_file(filename, bucket_name, filename)
        column_names = list_to_string(list(insert_dataframe))
        copy_query = """
                    SET SEARCH_PATH TO {schema}; COPY {table}({columns})
                    FROM 's3://redshiftstoragetransfer/{csv_name}'
                    CREDENTIALS 'aws_iam_role=arn:aws:iam::542960883369:role"""\
                    """/redshift_access_role' csv dateformat 'auto';"""\
                    .format(schema=schema_name,
                            table=table_name,
                            columns=column_names,
                            csv_name=filename)

        if self._table_exists(table_name):
            self.connection.execute(copy_query)
            self.connection.close()
        else:
            print "{table_name} does not exist in {schema_name}".\
                format(table_name=table_name, schema_name=schema_name)
        print "Saving {table_name} on {schema_name}".\
            format(table_name=table_name, schema_name=schema_name)

    def sql_dataframe(self, query):
        """Execute the query and return the queried results
           in a pandas dataframe.

        Args:
            query: query to execute
        Returns:
            df_result: pandas DataFrame of the queried result
        Raises:
            NA
        """
        try:
            df_result = pd.read_sql(query, self.connection)
        except TypeError:
            df_result = pd.read_sql(query.replace("%", "%%"), self.connection)

        return df_result

    def csv_s3_merge_redshift(self, insert_dataframe, table_name, key, schema_name):
        """Swap out the rows in a specified table in redshift with the rows in
           a specified dataframe by merging the two tables on a specified key.
           
        Args:
            insert_dataframe: pandas DataFrameto insert
            table_name: name of the table on redshift that will be modified
            key: name of the key variable to use
            schema_name: name of the schema that the table
                         you want to modify is in
        Returns:
            NA
        Raises:
            NA
        """
        if len(insert_dataframe) > 0:
            filename = table_name + "_temp.csv"
            bucket_name = "redshiftstoragetransfer"
            insert_dataframe.to_csv(filename, header=None, index=False)
            s3 = boto3.client('s3')
            s3.upload_file(filename, bucket_name, filename)
            column_names = list_to_string(list(insert_dataframe))
            temp_table_name = table_name + "_temp"
            merge_query = """
                            SET SEARCH_PATH TO {schema}; CREATE TEMP TABLE {temp_table_name} (LIKE {actual_table});

                                  COPY {temp_table_name}({columns}) FROM 's3://redshiftstoragetransfer/{csv_name}' CREDENTIALS 'aws_iam_role=arn:aws:iam::542960883369:role/redshift_access_role' csv dateformat 'auto';

                                  BEGIN TRANSACTION;

                                  DELETE FROM {actual_table}

                                  USING {temp_table_name}

                                  WHERE {temp_table_name}.{key} = {actual_table}.{key};

                                  INSERT INTO {actual_table}

                                  SELECT * FROM {temp_table_name};

                                  END TRANSACTION;

                                  DROP TABLE {temp_table_name};""".format(schema=schema_name, temp_table_name=temp_table_name, actual_table=table_name, columns=column_names, csv_name=filename, key=key)

            connection = self.engine.connect()
            connection.execute(merge_query)
            connection.close()
        else:
            pass

import pandas as pd
import os
import sys
import getpass
import argparse
import subprocess
from datetime import datetime
from datetime import timedelta
from bi_exceptions import BackfillException
from bi_exceptions import SnowflakeException
from botocore.exceptions import ClientError
import logging, sys
global username
username = getpass.getuser()
from io import BytesIO
import boto3
import botocore
from StringIO import StringIO
from bi_s3 import S3BI

def redshift_to_snowflake(schema, table):
    from bi_db import RedshiftConnection
    from bi_db import SnowflakeConnection
    from datetime import datetime
    rc = RedshiftConnection()
    sc = SnowflakeConnection()
    schema = schema.upper()
    table = table.upper()
    filepath = "s3://pitchbook-snowflake/schema={schema}/table={table}/"\
        "{date}/{schema}_{table}000".format(
            schema=schema,
            table=table,
            date=datetime.strftime(datetime.today(), "%Y-%m-%d")
        )
    rc.unload(database="dev", schema=schema, table=table)
    sc.load(db_name="BUSINESS_INTELLIGENCE",
            schema_name=schema,
            table_name=table,
            filepath=filepath,
            format="csv")
    return "done"

def flex_write(df, save_path, file_type="csv", **kwargs):
    if "s3" in kwargs:
        save_in_s3 = kwargs["s3"]
        if "bucket_name" in kwargs:
            bucket_name = kwargs["bucket_name"]
        else:
            bucket_name = "pitchbook-snowflake"
    else:
        save_in_s3 = False

    if save_in_s3 == True:
        s3bi = S3BI(bucket_name=bucket_name)
        s3bi.write_to_s3(df, save_path)
    else:
        if file_type.lower() == "h5":
            file_type = "hdf"
        flex_write = getattr(df, 'to_{}'.format(file_type))

        if file_type.lower() == "csv":
            flex_write(save_path, index=False)
        elif file_type.lower() == "hdf":
            flex_write(save_path, "w")

        print "{filetype} saved here: {savepath}".format(filetype=file_type,
                                                        savepath=save_path)

def flex_read(load_path, **kwargs):
    if "s3" in kwargs:
        load_from_s3 = kwargs["s3"]
        if "bucket_name" in kwargs:
            bucket_name = kwargs["bucket_name"]
        else:
            bucket_name = "pitchbook-snowflake"
    else:
        load_from_s3 = False

    if load_from_s3:
        s3bi = S3BI(bucket_name=bucket_name)
        df = s3bi.read_from_s3(load_path)
    else:
        file_type = load_path.split(".")[-1]
        flex_load = getattr(pd, 'read_{}'.format(file_type))
        df = flex_load(load_path)
        print "Loaded {filetype} from {loadpath}".format(filetype=file_type,
                                                            loadpath=load_path)
    return df

def timeit(method):
    def timed(*args, **kw):
        ts = time.time()
        result = method(*args, **kw)
        te = time.time()
        if 'log_time' in kw:
            name = kw.get('log_name', method.__name__.upper())
            kw['log_time'][name] = int((te - ts) * 1000)
        else:
            print '%r  %2.2f ms' % \
                  (method.__name__, (te - ts) * 1000)
        return result
    return timed

def backfill(filepath, start_date, end_date):
    shell_path = \
        "/home/{}/.local/lib/python2.7/site-packages/bi_tools/backfill.sh"\
            .format(username)
    try:
        subprocess.call([shell_path, filepath, start_date, end_date])
    except OSError:
        raise BackfillException("You do not have the shell script in" \
                                "{shellpath}".format(shellpath=shell_path))
    print "backfill job submitted"

def col_type_compare_and_match(df, db_df):
    if len(db_df) > 0:
        df_types = dict(df.dtypes)
        db_types = dict(db_df.dtypes)

        def convert_str_to_unicode(dict_to_convert):
            for key, value in dict_to_convert.items():
                # for now, assume any 'object' variable is unicode
                if value == object:
                    dict_to_convert[key] = type("str")
            return dict_to_convert

        df_types = convert_str_to_unicode(df_types)
        db_types = convert_str_to_unicode(db_types)

        cols_to_convert = []
        sharedKeys = set(df_types.keys()).intersection(db_types.keys())
        for key in sharedKeys:
            if df_types[key] != db_types[key]:
                cols_to_convert.append(key)

        for col in cols_to_convert:
            df[col] = df[col].astype(db_types[col])
    else:
        pass

    return df

def format_for_load(df):
    try:
        datetime_cols = [x for x in df.columns if "_date" in x.lower()]
    except AttributeError as e:
        new_header = df.iloc[0]
        df = df[1:]
        df.columns = new_header
        datetime_cols = [x for x in df.columns if "_date" in x.lower()]
    except:
        raise ValueError("check your s3 object input")
    for col in datetime_cols:
        df[col] = pd.to_datetime(df[col], errors = 'coerce')
    # if all values for the given column is na, then set it to string
    for col in df.columns:
        if df[col].isnull().all():
            df[col] = df[col].astype(str)
    return df

def make_directory(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)
    print "{dir} created".format(dir=directory)

def rename_w_snowflake_column_naming_convention(df):
    df.columns = [x.upper().replace(" ","_") for x in df.columns]
    return df
